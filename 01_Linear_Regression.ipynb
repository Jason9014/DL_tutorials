{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "To get familiar with automatic differentiation, we start by learning a simple linear regression model using Stochastic Gradient Descent (SGD).\n",
    "\n",
    "Recall that given a dataset $\\{(x_i, y_i)\\}_{i=0}^N$, with $x_i, y_i \\in \\mathbb{R}$, the objective of linear regression is to find two scalars $w$ and $b$ such that $y = w\\cdot x + b$ fits the dataset. In this tutorial we will learn $w$ and $b$ using SGD and a Mean Square Error (MSE) loss:\n",
    "\n",
    "$$L = \\frac{1}{N} \\sum_{i=0}^N (w\\cdot x_i + b - y_i)^2$$\n",
    "\n",
    "Starting from random values, parameters $w$ and $b$ will be updated at each iteration via the following rule:\n",
    "\n",
    "$$w_t = w_{t-1} - \\eta \\frac{\\partial L}{\\partial w}$$\n",
    "$$b_t = b_{t-1} - \\eta \\frac{\\partial L}{\\partial b}$$\n",
    "\n",
    "where $\\eta$ is the learning rate.\n",
    "\n",
    "## Placeholders and variables\n",
    "To implement and run this simple model, we will use the [Keras backend module](http://keras.io/backend/), which provides an abstraction over Theano and Tensorflow, two popular tensor manipulation libraries that provide automatic differentiation.\n",
    "\n",
    "First of all, we define the necessary variables and placeholders for our computational graph. Variables maintain state across executions of the computational graph, while placeholders are ways to feed the graph with external data.\n",
    "\n",
    "For the linear regression example, we need three variables: `w`, `b`, and the learning rate for SGD, `lr`. Two placeholders `x` and `target` are created to store $x_i$ and $y_i$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "# Placeholders and variables\n",
    "x = K.placeholder()\n",
    "target = K.placeholder()\n",
    "lr = K.variable(0.1)\n",
    "w = K.variable(np.random.rand())\n",
    "b = K.variable(np.random.rand())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model definition\n",
    "Now we can define the $y = w\\cdot x + b$ relation as well as the MSE loss in the computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define model and loss\n",
    "y = w * x + b\n",
    "loss = K.mean(K.square(y-target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, given the gradient of MSE wrt to `w` and `b`, we can define how we update the parameters via SGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"gradients_1/mul_grad/Reshape:0\", shape=TensorShape([]), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "grads = K.gradients(loss, [w,b])\n",
    "K.eval( grads[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grads = K.gradients(loss, [w,b])\n",
    "updates = [[w, w-lr*grads[0]], [b, b-lr*grads[1]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole model can be encapsulated in a `function`, which takes as input `x` and `target`, returns the current loss value and updates its parameter according to `updates`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = K.function(inputs=[x, target], outputs=[loss], updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Training is now just a matter of calling the `function` we have just defined. Each time `train` is called, indeed, `w` and `b` will be updated using the SGD rule.\n",
    "\n",
    "Having generated some random training data, we will feed the `train` function for several epochs and observe the values of `w`, `b`, and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate data\n",
    "np_x = np.random.rand(1000)\n",
    "np_target = 0.96*np_x + 0.24\n",
    "\n",
    "# Training\n",
    "loss_history = []\n",
    "for epoch in range(200):\n",
    "    current_loss = train([np_x, np_target])[0]\n",
    "    loss_history.append(current_loss)\n",
    "    if epoch % 20 == 0:\n",
    "        print \"Loss: %.03f, w, b: [%.02f, %.02f]\" % (current_loss, K.eval(w), K.eval(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the loss history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot loss history\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now have seen how a simple machine learning model can be implemented and trained using SGD and the Keras backend module. Recall that linear regression is indeed a simple neuron with a linear activation function\n",
    "\n",
    "Following the same principles and looking at the [Keras backend implementation](https://github.com/fchollet/keras/tree/master/keras/backend), you should be able to implement a basic Neural Network. In practice, we will use Keras to build and train Neural Networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
